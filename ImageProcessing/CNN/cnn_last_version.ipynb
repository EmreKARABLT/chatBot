{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:08:52.626632Z",
     "start_time": "2023-06-14T15:08:52.611670400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras import Sequential,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense , Input , MaxPool2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Rescaling,Dropout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class database():\n",
    "    def __init__(self , path_face_classifier , path_IMAGE_DIR):\n",
    "        self.face_classifier = cv2.CascadeClassifier(path_face_classifier)\n",
    "        self.IMAGE_DIR = path_IMAGE_DIR\n",
    "\n",
    "    def my_data(self , height , width ):\n",
    "\n",
    "        x_train = []\n",
    "        y_labels = []\n",
    "        for root, directories, files in os.walk(self.IMAGE_DIR):\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith(\"png\") or file.endswith(\"jpg\") :\n",
    "                    path = os.path.join(root, file)\n",
    "\n",
    "                    label = os.path.basename(os.path.dirname(path))\n",
    "\n",
    "                    image = cv2.imread(path)\n",
    "                    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "                    faces = self.face_classifier.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "\n",
    "\n",
    "                    for x, y, w, h in faces:\n",
    "                        rect_rgb = rgb[y:y + h, x:x + w]\n",
    "                        rect_rgb = cv2.resize(rect_rgb, (height, width), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                        x_train.append(rect_rgb)\n",
    "                        y_labels.append(label)\n",
    "\n",
    "        print(\"LFW data set created\")\n",
    "\n",
    "        return x_train , y_labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:08:53.020357500Z",
     "start_time": "2023-06-14T15:08:52.988925800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# from ImageProcessing.colorFeret.colorFeret import colorFeret\n",
    "# img_size = (384, 256)\n",
    "\n",
    "# _ , x_images , y_labels , n = colorFeret().load_dataset(max_angle = 70 , root = \"C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBot\\\\ImageProcessing\\\\colorFeret\")\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:08:53.761680800Z",
     "start_time": "2023-06-14T15:08:53.738740800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW data set created\n"
     ]
    },
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = (100 ,100, 3)\n",
    "\n",
    "face_classifier = \"C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBot\\\\ImageProcessing\\\\haarcascade_frontalface_default.xml\"\n",
    "IMAGE_DIR = \"C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBotLastPhase\\\\ImageProcessing\\\\Data\\\\Images\"\n",
    "\n",
    "x_images, y_labels = database(path_face_classifier=face_classifier , path_IMAGE_DIR = IMAGE_DIR).my_data( height=img_size[0],width=img_size[1])\n",
    "\n",
    "len(x_images)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:08:56.599975300Z",
     "start_time": "2023-06-14T15:08:54.052502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def data_aug(x_images, y_labels):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(len(x_images)):\n",
    "        img = x_images[i]\n",
    "        label = y_labels[i]\n",
    "        for j in range(3):\n",
    "\n",
    "            img_ = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1, 2))\n",
    "            img_ = tf.image.stateless_random_contrast(img_, lower=0.6, upper=1, seed=(1,3))\n",
    "            img_ = tf.image.stateless_random_flip_left_right(img_, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            img_ = tf.image.stateless_random_jpeg_quality(img_, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            img_ = tf.image.stateless_random_saturation(img_, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "\n",
    "            data.append(img_)\n",
    "            labels.append(label)\n",
    "    print(len(data), len(labels))\n",
    "    return data , labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T14:11:15.663329600Z",
     "start_time": "2023-06-14T14:11:15.644053700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0\n",
      "0\n",
      "[]\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 37\u001B[0m\n\u001B[0;32m     34\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(Y_filtered))\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(n)\n\u001B[1;32m---> 37\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mX_filtered\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m , cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgray\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "# def hist_eq(img):\n",
    "#\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "#     equalized = clahe.apply(img)\n",
    "#     noisy = random_noise(equalized, mode='gaussian', clip=True,mean=0 ,var=1)\n",
    "#     return noisy\n",
    "# for im,label in zip(x_images, y_labels):\n",
    "#     print(type(im), label)\n",
    "X,Y = data_aug(x_images,y_labels)\n",
    "#\n",
    "\n",
    "X_filtered = []\n",
    "Y_filtered = []\n",
    "\n",
    "min_number_of_images_exists_per_person = 90\n",
    "max_number_of_images_per_person_in_dataset = 270\n",
    "\n",
    "labels = np.array(y_labels)\n",
    "for x , y  in zip(X,Y):\n",
    "    if len(np.where(labels == y)[0]) >= min_number_of_images_exists_per_person and len(np.where(np.array(Y_filtered) == y)[0]) <= max_number_of_images_per_person_in_dataset:\n",
    "        random_rotate_angle = random.randint(-5,5)\n",
    "        random_flip_prob_threshold = 0.5\n",
    "        if random.random() < random_flip_prob_threshold:\n",
    "            x = np.flip(x , 1)\n",
    "        x = ndimage.rotate(x, random_rotate_angle , reshape=False)\n",
    "        X_filtered.append(x)\n",
    "        Y_filtered.append(y)\n",
    "print(len(X_filtered))\n",
    "print(len(Y_filtered))\n",
    "print(np.unique(Y_filtered))\n",
    "n = len(np.unique(Y_filtered))\n",
    "print(n)\n",
    "\n",
    "plt.imshow(X_filtered[10] , cmap='gray')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T14:11:18.861805400Z",
     "start_time": "2023-06-14T14:11:17.185916800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y has 0 samples: []",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m encoder \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n\u001B[1;32m----> 3\u001B[0m Y_encoded \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mY_filtered\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# for i , j in zip(Y_filtered,Y_encoded):\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#     print(i , \" - \" , j)\u001B[39;00m\n\u001B[0;32m      8\u001B[0m X_train, X_test, Y_train, Y_test \u001B[38;5;241m=\u001B[39m train_test_split(X_filtered,Y_encoded,test_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m , shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m , stratify\u001B[38;5;241m=\u001B[39mY_encoded)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    146\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:334\u001B[0m, in \u001B[0;36mLabelBinarizer.fit_transform\u001B[1;34m(self, y)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, y):\n\u001B[0;32m    315\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001B[39;00m\n\u001B[0;32m    316\u001B[0m \n\u001B[0;32m    317\u001B[0m \u001B[38;5;124;03m    The output of transform is sometimes referred to as\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;124;03m        will be of CSR format.\u001B[39;00m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(y)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:308\u001B[0m, in \u001B[0;36mLabelBinarizer.fit\u001B[1;34m(self, y)\u001B[0m\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultioutput target data is not supported with label binarization\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m     )\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _num_samples(y) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 308\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my has 0 samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m y)\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse_input_ \u001B[38;5;241m=\u001B[39m sp\u001B[38;5;241m.\u001B[39missparse(y)\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m unique_labels(y)\n",
      "\u001B[1;31mValueError\u001B[0m: y has 0 samples: []"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "\n",
    "Y_encoded = encoder.fit_transform(Y_filtered)\n",
    "# for i , j in zip(Y_filtered,Y_encoded):\n",
    "#     print(i , \" - \" , j)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_filtered,Y_encoded,test_size = 0.2 , shuffle=True , stratify=Y_encoded)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size = 0.30 , shuffle=True, stratify=Y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T14:11:19.064360200Z",
     "start_time": "2023-06-14T14:11:18.958811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m677\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m----> 3\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle((\u001B[43mY_train\u001B[49m[i]))\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(X_train[i] , cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgray\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(Y_filtered))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "i = 677\n",
    "print(i)\n",
    "plt.title((Y_train[i]))\n",
    "plt.imshow(X_train[i] , cmap=\"gray\")\n",
    "print(np.unique(Y_filtered))\n",
    "#\n",
    "# print(\"Data Created !!\")\n",
    "# print(type(X_train))\n",
    "# print(\"Length of training set \" , len(X_train) , len(Y_train))\n",
    "# print(\"Length of validation set \",len(X_val), len(Y_val))\n",
    "# print(\"Length of testing set \",len(X_test), len(Y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T14:11:22.544855600Z",
     "start_time": "2023-06-14T14:11:22.514318600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 4s 531ms/step - loss: 3.9876 - categorical_accuracy: 0.2015 - val_loss: 2.7984 - val_categorical_accuracy: 0.2226\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 3.2339 - categorical_accuracy: 0.1279 - val_loss: 2.1705 - val_categorical_accuracy: 0.2226\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.7540 - categorical_accuracy: 0.1309 - val_loss: 1.9892 - val_categorical_accuracy: 0.2226\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.5278 - categorical_accuracy: 0.1765 - val_loss: 1.9681 - val_categorical_accuracy: 0.2226\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.3433 - categorical_accuracy: 0.1971 - val_loss: 1.9503 - val_categorical_accuracy: 0.2226\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.3201 - categorical_accuracy: 0.1750 - val_loss: 1.9179 - val_categorical_accuracy: 0.2705\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 2.3268 - categorical_accuracy: 0.1750 - val_loss: 1.8826 - val_categorical_accuracy: 0.2808\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 2.2949 - categorical_accuracy: 0.1853 - val_loss: 1.8662 - val_categorical_accuracy: 0.2329\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.2132 - categorical_accuracy: 0.1868 - val_loss: 1.7917 - val_categorical_accuracy: 0.2911\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.1146 - categorical_accuracy: 0.2162 - val_loss: 1.7434 - val_categorical_accuracy: 0.3219\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.0318 - categorical_accuracy: 0.2500 - val_loss: 1.6744 - val_categorical_accuracy: 0.5240\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.9808 - categorical_accuracy: 0.2544 - val_loss: 1.6351 - val_categorical_accuracy: 0.4623\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.9039 - categorical_accuracy: 0.2853 - val_loss: 1.5562 - val_categorical_accuracy: 0.4829\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.8374 - categorical_accuracy: 0.3088 - val_loss: 1.4883 - val_categorical_accuracy: 0.4897\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.7218 - categorical_accuracy: 0.3706 - val_loss: 1.4650 - val_categorical_accuracy: 0.4623\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.6544 - categorical_accuracy: 0.3691 - val_loss: 1.4147 - val_categorical_accuracy: 0.4966\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 1.6736 - categorical_accuracy: 0.3500 - val_loss: 1.4209 - val_categorical_accuracy: 0.4486\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 1.6428 - categorical_accuracy: 0.3721 - val_loss: 1.3795 - val_categorical_accuracy: 0.4966\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.5712 - categorical_accuracy: 0.3897 - val_loss: 1.4273 - val_categorical_accuracy: 0.5240\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.6496 - categorical_accuracy: 0.3676 - val_loss: 1.3602 - val_categorical_accuracy: 0.5103\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.5799 - categorical_accuracy: 0.3676 - val_loss: 1.3289 - val_categorical_accuracy: 0.4658\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 1.5341 - categorical_accuracy: 0.3868 - val_loss: 1.3081 - val_categorical_accuracy: 0.5137\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 1.5041 - categorical_accuracy: 0.3971 - val_loss: 1.3050 - val_categorical_accuracy: 0.4932\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.5014 - categorical_accuracy: 0.3868 - val_loss: 1.2831 - val_categorical_accuracy: 0.5479\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.4381 - categorical_accuracy: 0.4279 - val_loss: 1.2568 - val_categorical_accuracy: 0.5788\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 1.3943 - categorical_accuracy: 0.4471 - val_loss: 1.2316 - val_categorical_accuracy: 0.5685\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.3674 - categorical_accuracy: 0.4529 - val_loss: 1.2029 - val_categorical_accuracy: 0.5822\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.3484 - categorical_accuracy: 0.4794 - val_loss: 1.1906 - val_categorical_accuracy: 0.5651\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.3332 - categorical_accuracy: 0.4824 - val_loss: 1.1802 - val_categorical_accuracy: 0.5616\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 1.2776 - categorical_accuracy: 0.4868 - val_loss: 1.1715 - val_categorical_accuracy: 0.5788\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 1.2934 - categorical_accuracy: 0.4926 - val_loss: 1.1561 - val_categorical_accuracy: 0.5993\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 1.2443 - categorical_accuracy: 0.5426 - val_loss: 1.1417 - val_categorical_accuracy: 0.5959\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.2471 - categorical_accuracy: 0.5279 - val_loss: 1.1281 - val_categorical_accuracy: 0.5925\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.2101 - categorical_accuracy: 0.5735 - val_loss: 1.1025 - val_categorical_accuracy: 0.6096\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.1947 - categorical_accuracy: 0.5735 - val_loss: 1.1057 - val_categorical_accuracy: 0.5616\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.2019 - categorical_accuracy: 0.5221 - val_loss: 1.0698 - val_categorical_accuracy: 0.5959\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.1486 - categorical_accuracy: 0.5897 - val_loss: 1.0495 - val_categorical_accuracy: 0.6267\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 1.1476 - categorical_accuracy: 0.5838 - val_loss: 1.0345 - val_categorical_accuracy: 0.6507\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.0903 - categorical_accuracy: 0.5809 - val_loss: 1.0313 - val_categorical_accuracy: 0.6370\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 1.1000 - categorical_accuracy: 0.6044 - val_loss: 1.0130 - val_categorical_accuracy: 0.6404\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.1130 - categorical_accuracy: 0.5897 - val_loss: 1.0752 - val_categorical_accuracy: 0.5445\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.1528 - categorical_accuracy: 0.5632 - val_loss: 1.0003 - val_categorical_accuracy: 0.6747\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 1.1001 - categorical_accuracy: 0.6015 - val_loss: 0.9865 - val_categorical_accuracy: 0.6575\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.0490 - categorical_accuracy: 0.6324 - val_loss: 0.9612 - val_categorical_accuracy: 0.6781\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.0084 - categorical_accuracy: 0.6294 - val_loss: 0.9362 - val_categorical_accuracy: 0.7055\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.9947 - categorical_accuracy: 0.6162 - val_loss: 0.9190 - val_categorical_accuracy: 0.6815\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.9668 - categorical_accuracy: 0.6471 - val_loss: 0.8916 - val_categorical_accuracy: 0.7021\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.9740 - categorical_accuracy: 0.6559 - val_loss: 0.8929 - val_categorical_accuracy: 0.6747\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.9181 - categorical_accuracy: 0.6676 - val_loss: 0.8708 - val_categorical_accuracy: 0.7021\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.9033 - categorical_accuracy: 0.6721 - val_loss: 0.8904 - val_categorical_accuracy: 0.6610\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.9127 - categorical_accuracy: 0.6618 - val_loss: 0.8564 - val_categorical_accuracy: 0.7021\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.9115 - categorical_accuracy: 0.6838 - val_loss: 0.8574 - val_categorical_accuracy: 0.7089\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.8562 - categorical_accuracy: 0.7015 - val_loss: 0.8810 - val_categorical_accuracy: 0.6610\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.8677 - categorical_accuracy: 0.6676 - val_loss: 0.8355 - val_categorical_accuracy: 0.7055\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.8388 - categorical_accuracy: 0.6971 - val_loss: 0.8073 - val_categorical_accuracy: 0.7021\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.8045 - categorical_accuracy: 0.7118 - val_loss: 0.7939 - val_categorical_accuracy: 0.7397\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.8037 - categorical_accuracy: 0.7235 - val_loss: 0.7856 - val_categorical_accuracy: 0.7123\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.7891 - categorical_accuracy: 0.7206 - val_loss: 0.7708 - val_categorical_accuracy: 0.7500\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.7583 - categorical_accuracy: 0.7529 - val_loss: 0.7690 - val_categorical_accuracy: 0.7260\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.7666 - categorical_accuracy: 0.7206 - val_loss: 0.7381 - val_categorical_accuracy: 0.7432\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.6952 - categorical_accuracy: 0.7706 - val_loss: 0.7454 - val_categorical_accuracy: 0.7466\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.7432 - categorical_accuracy: 0.7456 - val_loss: 0.7336 - val_categorical_accuracy: 0.7363\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.6842 - categorical_accuracy: 0.7765 - val_loss: 0.7040 - val_categorical_accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.6485 - categorical_accuracy: 0.7956 - val_loss: 0.7063 - val_categorical_accuracy: 0.7568\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.6316 - categorical_accuracy: 0.7868 - val_loss: 0.6798 - val_categorical_accuracy: 0.7637\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.6817 - categorical_accuracy: 0.7691 - val_loss: 0.6832 - val_categorical_accuracy: 0.7705\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.6355 - categorical_accuracy: 0.8044 - val_loss: 0.6779 - val_categorical_accuracy: 0.7637\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.6065 - categorical_accuracy: 0.8059 - val_loss: 0.6480 - val_categorical_accuracy: 0.7705\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.6119 - categorical_accuracy: 0.7956 - val_loss: 0.6483 - val_categorical_accuracy: 0.7671\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.5936 - categorical_accuracy: 0.8074 - val_loss: 0.6470 - val_categorical_accuracy: 0.7808\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.5653 - categorical_accuracy: 0.8324 - val_loss: 0.6156 - val_categorical_accuracy: 0.7842\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.5891 - categorical_accuracy: 0.8118 - val_loss: 0.6499 - val_categorical_accuracy: 0.7774\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.5375 - categorical_accuracy: 0.8500 - val_loss: 0.6028 - val_categorical_accuracy: 0.7979\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.4940 - categorical_accuracy: 0.8456 - val_loss: 0.5827 - val_categorical_accuracy: 0.7945\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.5003 - categorical_accuracy: 0.8471 - val_loss: 0.6523 - val_categorical_accuracy: 0.7603\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.5094 - categorical_accuracy: 0.8353 - val_loss: 0.5842 - val_categorical_accuracy: 0.8082\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.4704 - categorical_accuracy: 0.8485 - val_loss: 0.5700 - val_categorical_accuracy: 0.7945\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.4485 - categorical_accuracy: 0.8632 - val_loss: 0.5654 - val_categorical_accuracy: 0.8151\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.4305 - categorical_accuracy: 0.8618 - val_loss: 0.5735 - val_categorical_accuracy: 0.7945\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.4478 - categorical_accuracy: 0.8559 - val_loss: 0.5779 - val_categorical_accuracy: 0.7911\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.3957 - categorical_accuracy: 0.8912 - val_loss: 0.5477 - val_categorical_accuracy: 0.8356\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.4108 - categorical_accuracy: 0.8765 - val_loss: 0.5317 - val_categorical_accuracy: 0.8185\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.3828 - categorical_accuracy: 0.8897 - val_loss: 0.5055 - val_categorical_accuracy: 0.8390\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.3731 - categorical_accuracy: 0.8912 - val_loss: 0.5412 - val_categorical_accuracy: 0.8014\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.3491 - categorical_accuracy: 0.9000 - val_loss: 0.4963 - val_categorical_accuracy: 0.8253\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.3231 - categorical_accuracy: 0.9132 - val_loss: 0.5096 - val_categorical_accuracy: 0.8219\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.3144 - categorical_accuracy: 0.9221 - val_loss: 0.4993 - val_categorical_accuracy: 0.8219\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.2678 - categorical_accuracy: 0.9397 - val_loss: 0.5083 - val_categorical_accuracy: 0.8151\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.2825 - categorical_accuracy: 0.9353 - val_loss: 0.4679 - val_categorical_accuracy: 0.8493\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.2829 - categorical_accuracy: 0.9279 - val_loss: 0.4683 - val_categorical_accuracy: 0.8527\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2627 - categorical_accuracy: 0.9353 - val_loss: 0.4854 - val_categorical_accuracy: 0.8288\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.2551 - categorical_accuracy: 0.9294 - val_loss: 0.4298 - val_categorical_accuracy: 0.8630\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.2382 - categorical_accuracy: 0.9500 - val_loss: 0.4866 - val_categorical_accuracy: 0.8493\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.2417 - categorical_accuracy: 0.9471 - val_loss: 0.4326 - val_categorical_accuracy: 0.8630\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.2397 - categorical_accuracy: 0.9368 - val_loss: 0.4533 - val_categorical_accuracy: 0.8322\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.2338 - categorical_accuracy: 0.9485 - val_loss: 0.4367 - val_categorical_accuracy: 0.8493\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.1873 - categorical_accuracy: 0.9603 - val_loss: 0.4317 - val_categorical_accuracy: 0.8596\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.1898 - categorical_accuracy: 0.9603 - val_loss: 0.4253 - val_categorical_accuracy: 0.8596\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.1757 - categorical_accuracy: 0.9632 - val_loss: 0.4386 - val_categorical_accuracy: 0.8425\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.1599 - categorical_accuracy: 0.9721 - val_loss: 0.4092 - val_categorical_accuracy: 0.8733\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4064 - categorical_accuracy: 0.8566\n",
      "0.40642115473747253 0.8565573692321777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_image = Input(shape=(img_size[0],img_size[1], 3) , name='input')\n",
    "\n",
    "c1 = Conv2D(20, (4,4), activation='relu')(input_image)\n",
    "m1 = MaxPool2D((2,2))(c1)\n",
    "\n",
    "c2 = Conv2D(40, (3,3), activation='relu')(m1)\n",
    "m2 = MaxPool2D((2,2))(c2)\n",
    "\n",
    "c3 = Conv2D(60, (3,3), activation='relu')(m2)\n",
    "m3 = MaxPool2D((2,2))(c3)\n",
    "\n",
    "c4 = Conv2D(80, (2,2), activation='sigmoid')(m3)\n",
    "\n",
    "f1 = Flatten()(c4)\n",
    "\n",
    "d1 = Dense(512 , activation='sigmoid')(f1)\n",
    "dropout = Dropout(0.8)(d1)\n",
    "output = Dense(n , activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=[input_image], outputs=[output])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss= keras.losses.CategoricalCrossentropy(), metrics=keras.metrics.CategoricalAccuracy())\n",
    "\n",
    "epoch = 100\n",
    "batch_size =  256\n",
    "patience = 20\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss' , patience=patience, restore_best_weights=True  )\n",
    "history = model.fit(np.array(X_train), np.array(Y_train), epochs=epoch, batch_size =batch_size,validation_data=( np.array(X_val),np.array(Y_val)) ,callbacks=[callback] , use_multiprocessing = True)\n",
    "# history = model.fit(np.array(X_train), np.array(Y_train), epochs=100, batch_size = 64,validation_data=( np.array(X_val),np.array(Y_val)))\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=epoch ,batch_size=batch_size, validation_data=( X_val,Y_val), callbacks=[callback] )\n",
    "score, acc = model.evaluate(np.array(X_test), np.array(Y_test) )\n",
    "print(score,acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T13:13:27.308930100Z",
     "start_time": "2023-06-14T13:12:52.307502400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1/255., input_shape=(img_size[0],img_size[1],3), name='input'))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (4, 4) , activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) , strides=1))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=1))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "\n",
    "model.add(Dense(n, activation='softmax' , name='output'))\n",
    "\n",
    "# dot_img_file = 'C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBot\\\\ImageProcessing\\\\colorFeret\\\\model.png'\n",
    "# tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
    "# opt = tf.keras.optimizers.experimental.SGD(0.05 )\n",
    "# opt =  optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# opt =  optimizers.Adam(learning_rate=1e-4)\n",
    "# opt =  optimizers.SGD(learning_rate=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss= keras.losses.CategoricalCrossentropy(), metrics=keras.metrics.CategoricalAccuracy())\n",
    "\n",
    "epoch = 100\n",
    "batch_size =  32\n",
    "patience = 20\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss' , patience=patience, restore_best_weights=True  )\n",
    "history = model.fit(np.array(X_train), np.array(Y_train), epochs=epoch, batch_size =batch_size,validation_data=( np.array(X_val),np.array(Y_val)) ,callbacks=[callback] , use_multiprocessing = True)\n",
    "# history = model.fit(np.array(X_train), np.array(Y_train), epochs=100, batch_size = 64,validation_data=( np.array(X_val),np.array(Y_val)))\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=epoch ,batch_size=batch_size, validation_data=( X_val,Y_val), callbacks=[callback] )\n",
    "score, acc = model.evaluate(np.array(X_test), np.array(Y_test) )\n",
    "print(score,acc)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x00000228621D1B70>\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 97, 97, 20)        980       \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 48, 48, 20)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 46, 46, 40)        7240      \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 23, 23, 40)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 21, 21, 60)        21660     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 10, 10, 60)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 9, 9, 80)          19280     \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 6480)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               3318272   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,371,536\n",
      "Trainable params: 3,371,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 100, 100, 3), dtype=tf.float32, name='input'), name='input', description=\"created by layer 'input'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bpk_e.EMRE\\Desktop\\PROJECTS\\chatBot\\ImageProcessing\\fr_model_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bpk_e.EMRE\\Desktop\\PROJECTS\\chatBot\\ImageProcessing\\fr_model_3\\assets\n"
     ]
    }
   ],
   "source": [
    "# Saved TF model path\n",
    "print(model)\n",
    "model.summary()\n",
    "layer = model.get_layer('input')\n",
    "\n",
    "print(layer.input)\n",
    "# Directory to export new model\n",
    "\n",
    "model.save(\"C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBotLastPhase\\\\ImageProcessing\\\\Model\\\\fr_model_3\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T13:14:40.086805200Z",
     "start_time": "2023-06-14T13:14:38.174580700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Anna', 1: 'Colin_Powell', 2: 'Donald_Rumsfeld', 3: 'Emre_Karabulut', 4: 'Gabrijel', 5: 'George_W_Bush', 6: 'Gerhard_Schroeder', 7: 'Tony_Blair'}\n"
     ]
    }
   ],
   "source": [
    "# labels = np.flip(np.unique(Y_train,axis=0),1)\n",
    "\n",
    "labels = np.unique(Y_filtered)\n",
    "indices = range(0,len(labels))\n",
    "dict_ = {indices[i]: labels[i] for i in range(len(labels))}\n",
    "\n",
    "print(dict_)\n",
    "import json\n",
    "with open('C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBotLastPhase\\\\ImageProcessing\\\\Model\\\\labels.json', 'w') as f:\n",
    "    json.dump(dict_, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T13:14:23.851263600Z",
     "start_time": "2023-06-14T13:14:23.804769600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# index = 14\n",
    "# img = X_train[index]\n",
    "# label_encoded = Y_train[index]\n",
    "# label_index = np.where(label_encoded == 1 )[0]\n",
    "# label = labels[label_index]\n",
    "# print(label)\n",
    "img = cv2.imread('C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBot\\\\ImageProcessing\\\\lfw\\\\Emre_Karabulut\\\\Emre.1.jpg',0)\n",
    "# img = hist_eq(img)\n",
    "img = cv2.resize(img, img_size , cv2.INTER_AREA)\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "print(predictions * 100 )\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(labels[np.argmax(predictions[0] )], 100 * np.max(predictions[0]))\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "accuracy = history_dict['categorical_accuracy']\n",
    "val_accuracy = history_dict['val_categorical_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs, accuracy, label='Training accuracy')\n",
    "ax[0].plot(epochs, val_accuracy, label='Validation accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
    "ax[0].legend()\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs, loss_values, label='Training loss')\n",
    "ax[1].plot(epochs, val_loss_values, label='Validation loss')\n",
    "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].legend()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reset_gpu_memory()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# face_classifier = cv2.CascadeClassifier(\"C:\\\\Users\\\\bpk_e.EMRE\\\\Desktop\\\\PROJECTS\\\\chatBot\\\\ImageProcessing\\\\haarcascade_frontalface_default.xml\")\n",
    "# def face_cropped(img):\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "#\n",
    "#     if len(faces) == 0:\n",
    "#         print(\"NO FACE DETECTED!!\")\n",
    "#         return None\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         margin = 10\n",
    "#         cropped_face = img[y-margin:y + h+margin, x-margin:x + w+margin]\n",
    "#     return cropped_face\n",
    "#\n",
    "#\n",
    "# def generate_dataset(name):\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#\n",
    "#     img_id = 0\n",
    "#     print(\"usao\")\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if face_cropped(frame) is not None:\n",
    "#             img_id += 1\n",
    "#             face = cv2.resize(face_cropped(frame), (224, 224))\n",
    "#             face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "#\n",
    "#             file_name_path = \"Images\\\\\" + name + \"_\" + str(img_id) + '.jpg'\n",
    "#             cv2.imwrite(file_name_path, face)\n",
    "#             # cv2.putText(face, str(img_id), (224, 224), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "#\n",
    "#             cv2.imshow(\"Cropped_Face\", face)\n",
    "#             if cv2.waitKey(1) == 13 or int(img_id) == 100:\n",
    "#                 break\n",
    "#\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     print(\"Collecting samples is completed !!!\")\n",
    "# generate_dataset(\"emre\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
